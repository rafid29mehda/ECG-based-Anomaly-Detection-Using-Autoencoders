# ECG-based-Anomaly-Detection-Using-Autoencoders

---

### Overview of the Script
The script uses an **autoencoder**, a type of neural network, to detect anomalies (abnormal heartbeats) in ECG data. It processes data from two datasets: the **MIT-BIH Arrhythmia Database** (containing normal and abnormal heartbeats) and the **MIT-BIH Normal Sinus Rhythm Database** (containing only normal heartbeats). The autoencoder learns to reconstruct normal ECG signals and flags signals with high reconstruction errors as anomalies (abnormal). The script includes data loading, preprocessing, model building, training, and evaluation, with visualizations to interpret results.

---

### Section-by-Section Explanation

#### 1. **Header and Initial Setup**
```python
# -*- coding: utf-8 -*-
"""ECG-Based Anomaly Detection using Autoencoders

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=...
"""
```
**Explanation:**
- The first line, `# -*- coding: utf-8 -*-`, specifies the file’s character encoding, ensuring it can handle special characters (though not critical here).
- The comment indicates the script was created in **Google Colab**, a cloud-based Python environment popular for machine learning tasks due to its free GPU support and pre-installed libraries.
- The link points to the original Colab notebook, which is useful for tracking the source but not needed for execution.

**Purpose:**
This is metadata for the script, providing context about its origin and purpose. It doesn’t affect the code’s functionality but helps you understand it’s designed for ECG anomaly detection using autoencoders.

**Example:**
Think of this as the cover page of a book. It tells you the title (“ECG-Based Anomaly Detection using Autoencoders”) and where it was created (Colab). For your PhD research, you might save similar scripts in Colab or Jupyter notebooks to organize your ECG-related experiments.

---

#### 2. **Kaggle Data Import**
```python
import kagglehub
shymammoth_mitbih_normal_sinus_rhythm_database_path = kagglehub.dataset_download('shymammoth/mitbih-normal-sinus-rhythm-database')
klmsathishkumar_mit_bih_arrhythmia_database_path = kagglehub.dataset_download('klmsathishkumar/mit-bih-arrhythmia-database')

print('Data source import complete.')
```
**Explanation:**
- **What it does**: This block downloads two datasets from Kaggle using the `kagglehub` library:
  - **MIT-BIH Normal Sinus Rhythm Database**: Contains ECG recordings of normal heartbeats.
  - **MIT-BIH Arrhythmia Database**: Contains ECG recordings with both normal and abnormal (arrhythmic) heartbeats.
- **kagglehub.dataset_download**: Automatically downloads the datasets to a local path in the Colab environment and returns the file paths.
- **Purpose**: These datasets provide the raw ECG signals needed for training the autoencoder to distinguish normal from abnormal heartbeats.
- **Why it matters**: For your PhD in Biomedical Signal Processing, working with standard datasets like MIT-BIH is crucial, as they are widely used in ECG research, ensuring your results are comparable to other studies.

**Example:**
Imagine you’re borrowing books from a library (Kaggle) for your research. The `kagglehub` library is like a librarian who fetches the books (datasets) for you. The datasets are stored in a folder on your computer (or Colab’s temporary storage), and the `print` statement confirms the books have arrived.

**For Beginners:**
- **Kaggle**: A platform hosting datasets and competitions. The MIT-BIH datasets are standard for ECG research.
- **Dataset structure**: Each dataset contains ECG signal files (`.dat`), annotation files (`.atr`), and header files (`.hea`) for each patient. You’ll learn to read these using the `wfdb` library later.

---

#### 3. **Text Cell: Introduction to ECG-Based Anomaly Detection**
```markdown
**ECG-Based Arrhythmias Anomaly Detection using Autoencoders**

ECG-Based Arrhythmias Anomaly Detection using Autoencoders An autoencoder is a type of neural network model that attempts to learn a compact representation of the input. Although they are an unsupervised learning method, they are technically trained using supervised learning methods, which are referred to as self-supervised. Typically, they are trained as part of a larger model that seeks to duplicate the input. The goal of an autoencoder is to train the network to capture the most important parts of the input image to learn a lower-dimensional representation for higher-dimensional data, typically for dimensionality reduction.
```
**Explanation:**
- **What is an autoencoder?** An autoencoder is a neural network that learns to copy its input to its output. It has two parts:
  - **Encoder**: Compresses the input (e.g., ECG signal) into a smaller, dense representation (called the latent space).
  - **Decoder**: Reconstructs the input from this compressed representation.
- **Why use it for ECG?** The autoencoder is trained on **normal** ECG signals. If it reconstructs a signal poorly (high error), the signal is likely **abnormal** (an anomaly), as the model isn’t familiar with abnormal patterns.
- **Unsupervised vs. Self-Supervised**:
  - **Unsupervised**: No labels are needed to train the autoencoder to reconstruct data.
  - **Self-Supervised**: The input itself acts as the “label” (the model tries to reproduce the input).
- **Purpose in this project**: The autoencoder learns the patterns of normal ECG signals. Abnormal signals (arrhythmias) will have higher reconstruction errors, allowing anomaly detection.
- **For your PhD**: Autoencoders are popular in Biomedical Signal Processing for tasks like anomaly detection, denoising, or feature extraction in ECG, EEG, or other biosignals.

**Example:**
Imagine you’re teaching a child to draw a perfect circle by showing them many examples of circles. The child learns to draw circles well but struggles with squares. Similarly, the autoencoder learns to “draw” (reconstruct) normal ECG signals. If you give it an abnormal ECG (like a square), it draws it poorly, and the difference (error) indicates an anomaly.

**Key Terms for Beginners:**
- **ECG (Electrocardiogram)**: A recording of the heart’s electrical activity, showing heartbeats as waves (P, QRS, T).
- **Anomaly**: An abnormal heartbeat (e.g., arrhythmia) that differs from a normal heartbeat.
- **Dimensionality Reduction**: Reducing a large dataset (e.g., 140 ECG data points) to a smaller representation (e.g., 16 values) while keeping key features.

---

#### 4. **Installing Required Libraries**
```python
!pip install wfdb
import wfdb
pip install matplotlib==3.1.3
```
**Explanation:**
- **`!pip install wfdb`**: Installs the `wfdb` (WaveForm DataBase) library, which is essential for reading MIT-BIH dataset files (`.dat`, `.atr`, `.hea`).
- **`import wfdb`**: Imports the library to use its functions for loading ECG signals and annotations.
- **`pip install matplotlib==3.1.3`**: Installs a specific version (3.1.3) of `matplotlib`, a plotting library for visualizing ECG signals and results. The specific version ensures compatibility with the script.
- **Note**: The syntax `pip install` (without `!`) is incorrect in Colab; it should be `!pip install`. This appears to be a typo in the script, but Colab will ignore it.

**Purpose:**
These libraries are tools for your research:
- `wfdb`: Reads ECG data in the MIT-BIH format.
- `matplotlib`: Creates plots (e.g., ECG waveforms, loss curves).

**Example:**
Think of `wfdb` as a specialized tool for reading medical charts (ECG files), and `matplotlib` as your sketchbook for drawing graphs. Installing them is like setting up your lab equipment before starting an experiment.

**For Beginners:**
- **Why wfdb?** MIT-BIH datasets use a unique format (`.dat` for signals, `.atr` for annotations). The `wfdb` library understands this format, making it easy to extract signals and labels.
- **Why a specific matplotlib version?** Older versions may avoid compatibility issues with other libraries in the Colab environment.

---

#### 5. **Importing Additional Libraries**
```python
# Importing Required Packages
import keras
from keras import layers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import random
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
import copy
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.metrics import precision_score, recall_score, accuracy_score
from tensorflow.keras import layers, losses
from tensorflow.keras.models import Model
import scipy.io
from scipy.io import savemat
# Random Initialization
random.seed(42)
```
**Explanation:**
This block imports libraries needed for data processing, machine learning, and visualization:
- **keras, tensorflow**: Libraries for building and training the autoencoder neural network.
  - `keras.layers`: Provides building blocks for neural networks (e.g., Dense layers).
  - `tensorflow.keras.utils.to_categorical`: Converts labels to one-hot encoding (not used here, likely included by default).
  - `tensorflow.keras.losses`: Defines loss functions (e.g., mean absolute error).
  - `tensorflow.keras.models.Model`: Allows creating custom models like the autoencoder.
- **pandas, numpy**: For data manipulation.
  - `pandas`: Handles data in tabular form (DataFrames).
  - `numpy`: Performs numerical operations on arrays (e.g., ECG signals).
- **matplotlib, seaborn, pylab**: For plotting.
  - `matplotlib.pyplot`: Creates basic plots (e.g., ECG waveforms).
  - `seaborn`: Enhances visualizations (e.g., histograms).
  - `pylab, matplotlib.rc`: Configures plot settings (e.g., figure size).
- **sklearn**: For data preprocessing and evaluation.
  - `train_test_split`: Splits data into training and testing sets.
  - `precision_score, recall_score, accuracy_score`: Metrics to evaluate model performance.
- **torch**: PyTorch library for GPU acceleration. The `device` line checks if a GPU is available; if not, it uses the CPU. (Note: PyTorch is imported but not used in the script, likely a leftover.)
- **scipy.io, savemat**: For saving data in MATLAB format (not used in this script).
- **random.seed(42)**: Sets a random seed for reproducibility, ensuring random operations (e.g., data splitting) yield the same results each time.

**Purpose:**
These libraries are the backbone of the project, enabling data loading, preprocessing, model building, training, and evaluation. For your PhD, mastering these libraries (especially `wfdb`, `tensorflow`, and `pandas`) will be essential for ECG research.

**Example:**
Imagine you’re cooking a complex dish. You need ingredients (data), tools (libraries like `pandas`, `tensorflow`), and a recipe (the script). The `random.seed(42)` is like setting the oven to a specific temperature to ensure consistent results each time you cook.

**For Beginners:**
- **Why so many libraries?** Each library has a specific role:
  - `pandas` organizes data like a spreadsheet.
  - `numpy` handles math operations on large datasets.
  - `tensorflow` builds and trains the neural network.
  - `matplotlib` and `seaborn` create graphs to visualize your findings.
- **Random Seed**: Setting `random.seed(42)` ensures that if you run the script multiple times, the results (e.g., train-test split) are identical, which is critical for reproducible research.

---

#### 6. **Checking Input Directory**
```python
import os
print(os.listdir("../input"))
```
**Explanation:**
- **What it does**: The `os.listdir("../input")` function lists the files and folders in the `../input` directory, where Kaggle datasets are typically stored in Colab.
- **Purpose**: This helps verify that the datasets (MIT-BIH Arrhythmia and Normal Sinus Rhythm) were downloaded correctly by `kagglehub`.
- **Output**: It will print folder names like `mit-bih-arrhythmia-database` and `mitbih-normal-sinus-rhythm-database`.

**Example:**
Think of this as checking your fridge to see if the ingredients (datasets) you ordered are there. The output might look like:
```
['mit-bih-arrhythmia-database', 'mitbih-normal-sinus-rhythm-database']
```
This confirms the datasets are available for processing.

**For Beginners:**
- **Why `../input`?** In Kaggle or Colab, datasets are stored in a standard directory called `input`. The `..` means “go up one folder” from the current working directory.
- **Use in research**: Always verify your data is accessible before processing, as missing files can cause errors.

---

#### 7. **Text Cell: Abnormal ECG Data (MIT-BIH Arrhythmia Dataset)**
```markdown
# Abnormal ECG Data

The MIT-BIH Arrhythmia dataset (MIT-BIH-ARR) is mainly composed of 48 two-channel ambulatory ECG recordings sampled at 360Hz. Each recording lasts about 30 minutes. The data was gathered from 47 people. The participants were 25 men and 22 women, ranging in age from 32 to 89 years old. This dataset comprises recordings with various degrees of arrhythmias and also recordings with normal sinus rhythm. Professional reference annotations are included in each recording in the form of rhythm and beat annotations.
```
**Explanation:**
- **MIT-BIH Arrhythmia Dataset**: A widely used dataset in ECG research, containing 48 recordings from 47 patients (one patient has two recordings).
- **Key Features**:
  - **Two-channel**: Each recording has two ECG signals (e.g., different electrode placements, like Lead I and Lead II).
  - **Sampling rate**: 360 Hz (360 samples per second), meaning each second of ECG data has 360 data points.
  - **Duration**: Each recording is ~30 minutes, so at 360 Hz, that’s about 30 × 60 × 360 = 648,000 data points per channel.
  - **Participants**: 25 men, 22 women, aged 32–89, providing diversity in heart conditions.
  - **Content**: Includes normal heartbeats and various arrhythmias (abnormal heartbeats).
  - **Annotations**: Each heartbeat is labeled (e.g., “N” for normal, “V” for ventricular ectopic beat) by experts, stored in `.atr` files.
- **Purpose**: This dataset provides both normal and abnormal ECG signals, allowing the autoencoder to learn normal patterns and detect anomalies (arrhythmias).
- **For your PhD**: This dataset is a gold standard in ECG research. Understanding its structure and annotations is critical for your work in Biomedical Signal Processing.

**Example:**
Imagine a 30-minute video of a heart’s electrical activity, recorded from two angles (channels). Every heartbeat in the video is tagged by a doctor with labels like “normal” or “abnormal.” The dataset is like a collection of 48 such videos from different people, and you’ll use these to train your model to spot unusual heartbeats.

**Key Terms for Beginners:**
- **Sampling Rate (360 Hz)**: The ECG machine records 360 data points per second, capturing the heart’s electrical signals in detail.
- **Arrhythmia**: An abnormal heartbeat (e.g., too fast, too slow, or irregular).
- **Annotations**: Labels for each heartbeat, like “N” (normal) or “V” (ventricular ectopic), stored in `.atr` files.

---

#### 8. **Loading and Summarizing Annotations**
```python
data = '../input/mit-bih-arrhythmia-database/'
patients = ['100','101','102','103','104','105','106','107',
           '108','109','111','112','113','114','115','116',
           '117','118','119','121','122','123','124','200',
           '201','202','203','205','207','208','209','210',
           '212','213','214','215','217','219','220','221',
           '222','223','228','230','231','232','233','234']

# Creating a Empty Dataframe
dataframe = pd.DataFrame()

# Reading all .atr files
for pts in patients:
    file = data + pts
    annotation = wfdb.rdann(file, 'atr')
    sym = annotation.symbol
    values, counts = np.unique(sym, return_counts=True)
    df_sub = pd.DataFrame({'symbol':values, 'Counts':counts, 'Patient Number':[pts]*len(counts)})
    dataframe = pd.concat([dataframe, df_sub], axis=0)

ax = sns.countplot(dataframe.symbol)
dataframe
```
**Explanation:**
- **What it does**: This code reads annotation files (`.atr`) for each patient in the MIT-BIH Arrhythmia Database, counts the occurrences of each heartbeat symbol (e.g., “N”, “V”), and stores the results in a DataFrame. It also visualizes the symbol distribution.
- **Step-by-step**:
  - **Path setup**: `data = '../input/mit-bih-arrhythmia-database/'` points to the dataset folder.
  - **Patient list**: Defines 48 patient IDs (e.g., “100”, “101”).
  - **Empty DataFrame**: `dataframe = pd.DataFrame()` creates an empty table to store annotation data.
  - **Loop over patients**:
    - `file = data + pts`: Constructs the file path (e.g., `../input/mit-bih-arrhythmia-database/100`).
    - `annotation = wfdb.rdann(file, 'atr')`: Reads the `.atr` file, which contains heartbeat annotations.
    - `sym = annotation.symbol`: Extracts the symbols (e.g., “N”, “V”) for each heartbeat.
    - `values, counts = np.unique(sym, return_counts=True)`: Counts how many times each symbol appears (e.g., “N”: 2000, “V”: 50).
    - `df_sub = pd.DataFrame({'symbol':values, 'Counts':counts, 'Patient Number':[pts]*len(counts)})`: Creates a small DataFrame for the patient, with columns for symbols, their counts, and patient ID.
    - `dataframe = pd.concat([dataframe, df_sub], axis=0)`: Adds the patient’s data to the main DataFrame.
  - **Visualization**: `ax = sns.countplot(dataframe.symbol)` creates a bar plot showing the frequency of each symbol across all patients.
  - **Display DataFrame**: `dataframe` shows the table with columns `symbol`, `Counts`, and `Patient Number`.

**Purpose:**
This summarizes the types and frequencies of heartbeats in the dataset, helping you understand the data’s composition (e.g., how many normal vs. abnormal beats). The plot visualizes the distribution, highlighting class imbalance (e.g., more “N” than “V”).

**Example:**
Suppose patient “100” has 2000 “N” (normal) beats and 50 “V” (ventricular ectopic) beats. The code counts these, stores them in a table, and repeats for all patients. The final DataFrame might look like:
```
   symbol  Counts  Patient Number
0      N    2000           100
1      V      50           100
2      N    1800           101
3      A     100           101
...
```
The bar plot shows that “N” (normal) is the most common symbol, indicating an imbalanced dataset where normal beats dominate.

**For Beginners:**
- **Why annotations?** Annotations label each heartbeat, telling you if it’s normal or abnormal, which is crucial for training and evaluating your model.
- **Why countplot?** It shows which heartbeat types are common or rare, helping you identify data imbalance (a common issue in ECG research).
- **Use in PhD**: Analyzing annotation distributions is a standard step in ECG research to understand dataset characteristics before modeling.

---

#### 9. **Classifying Heartbeats**
```python
# Non Beat Symbols
nonbeat = ['[','!',']','x','(',')','p','t','u','`',
           '\'','^','|','~','+','s','T','*','D','=','"','@','Q','?']

# Abnormal Beat Symbols
abnormal = ['L','R','V','/','A','f','F','j','a','E','J','e','S']

# Normal Beat Symbols
normal = ['N']

# Classifying normal, abnormal or nonbeat
dataframe['category'] = -1
dataframe.loc[dataframe.symbol == 'N', 'category'] = 0
dataframe.loc[dataframe.symbol.isin(abnormal), 'category'] = 1

dataframe.groupby('category').Counts.sum()

# removing the non-beat from dataframe
dataframe = dataframe.loc[~((dataframe['category']==-1))]
dataframe.groupby('category').Counts.sum()
```
**Explanation:**
- **What it does**: This code categorizes heartbeats into three types: **normal**, **abnormal**, and **non-beat**, then removes non-beats from the DataFrame and summarizes the counts of normal and abnormal beats.
- **Step-by-step**:
  - **Symbol lists**:
    - `nonbeat`: Symbols for non-beat events (e.g., noise, artifacts), not actual heartbeats.
    - `abnormal`: Symbols for arrhythmic beats (e.g., “V” for ventricular ectopic, “A” for atrial premature).
    - `normal`: Only “N” for normal heartbeats.
  - **Categorization**:
    - `dataframe['category'] = -1`: Adds a new column, `category`, initialized to -1 (non-beat).
    - `dataframe.loc[dataframe.symbol == 'N', 'category'] = 0`: Sets category to 0 for normal beats (“N”).
    - `dataframe.loc[dataframe.symbol.isin(abnormal), 'category'] = 1`: Sets category to 1 for abnormal beats.
  - **Summarize counts**: `dataframe.groupby('category').Counts.sum()` shows the total number of beats per category (e.g., normal: 90,000, abnormal: 10,000).
  - **Remove non-beats**: `dataframe = dataframe.loc[~((dataframe['category']==-1))]` keeps only normal (0) and abnormal (1) beats.
  - **Final summary**: Repeats the grouping to show counts after removing non-beats.

**Purpose:**
This simplifies the dataset by focusing on normal and abnormal heartbeats, ignoring non-beat events (e.g., noise). It quantifies the imbalance between normal and abnormal beats, which is critical for anomaly detection.

**Example:**
Suppose the initial DataFrame has:
```
   symbol  Counts  Patient Number  category
0      N    2000           100         0
1      V      50           100         1
2      !     100           100        -1
3      N    1800           101         0
4      A     100           101         1
```
After categorization, `category` is set to 0 (normal), 1 (abnormal), or -1 (non-beat). The first `groupby` might show:
```
category
-1     1000  # non-beats
 0    90000  # normal beats
 1    10000  # abnormal beats
```
After removing non-beats, the DataFrame only includes rows with `category` 0 or 1, and the final `groupby` shows:
```
category
 0    90000  # normal beats
 1    10000  # abnormal beats
```

**For Beginners:**
- **Why remove non-beats?** Non-beats (e.g., noise, pacing signals) aren’t relevant for detecting heart-related anomalies, so they’re excluded.
- **Why categorize?** The autoencoder needs to distinguish normal (0) from abnormal (1) beats. This step prepares the labels.
- **Imbalance**: Normal beats often outnumber abnormal ones, a common challenge in ECG research that you’ll address in your PhD.
- 


### Detailed Explanation of Each Symbol List

#### 1. **Non-Beat Symbols**
```python
nonbeat = ['[','!',']','x','(',')','p','t','u','`',
           '\'','^','|','~','+','s','T','*','D','=','"','@','Q','?']
```

**What Are Non-Beat Symbols?**
- These symbols represent **events that are not heartbeats** in the ECG signal. They include artifacts, noise, pacing signals, or other non-cardiac events.
- In the MIT-BIH Arrhythmia Database, these are annotations for things like:
  - **Noise or artifacts**: Signals caused by movement, electrode issues, or external interference (e.g., `~` for noise).
  - **Pacing signals**: Artificial signals from a pacemaker (e.g., `[` or `]` for pacing markers).
  - **Other events**: Comments or undefined events (e.g., `?`).
- **Examples of Non-Beat Symbols**:
  - `[`, `]`: Indicate the start/end of a pacing spike (from a pacemaker).
  - `~`: Noise or baseline drift in the ECG signal.
  - `p`: Paced beat (a heartbeat triggered by a pacemaker).
  - `Q`, `?`: Unclassified or questionable events.
- **Role in the Script**:
  - Non-beat symbols are **excluded** from the dataset because the autoencoder focuses on detecting anomalies in actual heartbeats (normal or abnormal).
  - The script later filters out these symbols using:
    ```python
    dataframe = dataframe.loc[~((dataframe['category']==-1))]
    ```
    This ensures only heartbeat-related data (normal or abnormal) is used for training and testing.

**Example**:
- Imagine an ECG signal with a spike caused by a patient moving, labeled as `~`. This isn’t a heartbeat, so it’s marked as a non-beat (`category = -1`) and removed from the analysis.
- In a 30-minute ECG recording, you might find 100 `~` symbols due to noise, but these won’t help the autoencoder learn about heartbeats, so they’re ignored.
---

#### 2. **Abnormal Beat Symbols**
```python
abnormal = ['L','R','V','/','A','f','F','j','a','E','J','e','S']
```

**What Are Abnormal Beat Symbols?**
- These symbols represent **arrhythmic heartbeats** (abnormal beats) that deviate from a normal heart rhythm. Arrhythmias are irregular or abnormal heartbeats that may indicate heart conditions.
- Each symbol corresponds to a specific type of arrhythmia, as defined by the MIT-BIH database (based on AAMI standards). Here’s what each symbol means:
  - **L**: Left bundle branch block beat (abnormal conduction in the left heart).
  - **R**: Right bundle branch block beat (abnormal conduction in the right heart).
  - **V**: Premature ventricular contraction (PVC, an early ventricular beat).
  - **/**: Paced beat (often considered abnormal in natural heart rhythm analysis).
  - **A**: Atrial premature beat (early atrial contraction).
  - **f**: Fusion of ventricular and normal beat (a mix of normal and abnormal).
  - **F**: Fusion of paced and normal beat.
  - **j**: Nodal (junctional) escape beat (backup pacemaker in the AV node).
  - **a**: Aberrated atrial premature beat (distorted atrial beat).
  - **E**: Ventricular escape beat (backup ventricular beat).
  - **J**: Nodal (junctional) premature beat.
  - **e**: Atrial escape beat.
  - **S**: Supraventricular premature beat (originates above the ventricles).
- **Role in the Script**:
  - These are labeled as `category = 1` in the DataFrame:
    ```python
    dataframe.loc[dataframe.symbol.isin(abnormal), 'category'] = 1
    ```
  - Abnormal beats are used to test the autoencoder’s ability to detect anomalies. Since the autoencoder is trained on normal beats (`N`), it should have a higher reconstruction error for these abnormal beats, indicating an anomaly.
- **Characteristics**:
  - Abnormal beats have different ECG waveforms (e.g., wider QRS complex for PVCs, missing P waves for junctional beats).
  - They are less common than normal beats, leading to an imbalanced dataset (e.g., 90,000 normal vs. 10,000 abnormal beats).

**Example**:
- A **V** (premature ventricular contraction) is an early heartbeat originating from the ventricles, often seen as a wide, bizarre QRS complex on the ECG. In the script, a segment labeled `V` is marked as `category = 1` and used to evaluate if the autoencoder flags it as an anomaly due to poor reconstruction.
- Suppose a patient’s ECG has 50 `V` beats. These are extracted as 140-point segments and labeled as abnormal for testing.

---

#### 3. **Normal Beat Symbols**
```python
normal = ['N']
```

**What Are Normal Beat Symbols?**
- The `N` symbol represents a **normal sinus rhythm heartbeat**, which is a healthy, regular heartbeat originating from the sinoatrial (SA) node.
- **Characteristics**:
  - A normal ECG beat includes a P wave (atrial contraction), QRS complex (ventricular contraction), and T wave (ventricular relaxation).
  - It has a consistent shape and timing, with a heart rate typically 60–100 beats per minute.
- **Role in the Script**:
  - Normal beats are labeled as `category = 0`:
    ```python
    dataframe.loc[dataframe.symbol == 'N', 'category'] = 0
    ```
  - The autoencoder is **trained on normal beats** (`normal_train_data`) to learn their patterns. It should reconstruct these beats with low error, while abnormal beats (or non-beats, if not filtered) will have higher errors.
- **Why only `N`?** In the MIT-BIH Arrhythmia Database, `N` is the standard annotation for a normal sinus rhythm beat. Other datasets might use additional symbols for normal beats, but this script simplifies to `N`.

**Example**:
- A normal beat (`N`) in a patient’s ECG might occur 2,000 times in a 30-minute recording. Each `N` beat is extracted as a 140-point segment, labeled `category = 0`, and used to train the autoencoder to recognize the typical ECG waveform (e.g., clear P, QRS, T waves).
- The autoencoder learns to reconstruct these normal segments accurately, so any deviation (e.g., an abnormal `V` beat) results in a high reconstruction error, flagging it as an anomaly


---

#### 10. **Loading ECG Data Function**
```python
def load_ecg(file):
    record = wfdb.rdrecord(file)
    annotation = wfdb.rdann(file, 'atr')
    p_signal = record.p_signal
    atr_sym = annotation.symbol
    atr_sample = annotation.sample
    return p_signal, atr_sym, atr_sample
```
**Explanation:**
- **What it does**: Defines a function to load an ECG recording and its annotations for a given patient.
- **Parameters**:
  - `file`: Path to the patient’s data (e.g., `../input/mit-bih-arrhythmia-database/100`).
- **Steps**:
  - `record = wfdb.rdrecord(file)`: Loads the ECG signal from the `.dat` file (and metadata from `.hea`).
  - `annotation = wfdb.rdann(file, 'atr')`: Loads the annotation file (`.atr`).
  - `p_signal = record.p_signal`: Extracts the ECG signal (a 2D array with shape `[num_samples, num_channels]`).
  - `atr_sym = annotation.symbol`: Gets the list of heartbeat symbols (e.g., [“N”, “V”, “N”]).
  - `atr_sample = annotation.sample`: Gets the sample indices where heartbeats occur (e.g., [100, 300, 500]).
- **Returns**: The ECG signal, symbols, and sample indices.
- **Purpose**: This function retrieves raw ECG data and annotations, which are processed further to create the dataset.

**Example:**
For patient “100”:
- `file = '../input/mit-bih-arrhythmia-database/100'`.
- `record.p_signal`: Returns a NumPy array of shape `[648000, 2]` (30 minutes × 360 Hz × 2 channels).
- `annotation.symbol`: Returns `[“N”, “N”, “V”, “N”]`, indicating heartbeat types.
- `annotation.sample`: Returns `[100, 300, 500, 700]`, indicating where heartbeats occur in the signal.
The function returns these three items for further processing.

**For Beginners:**
- **ECG Signal**: A time series of voltage values representing the heart’s electrical activity. For MIT-BIH, it’s recorded from two leads (channels).
- **Annotations**: Markers indicating where heartbeats occur and their types.
- **Use in PhD**: This function is a standard way to load ECG data in research, especially for MIT-BIH datasets.

---

#### 11. **Building X, Y Matrices**
```python
def build_XY(p_signal, df_ann, num_cols, normal):
    num_rows = len(df_ann)
    X = np.zeros((num_rows, num_cols))
    Y = np.zeros((num_rows, 1))
    sym = []
    max_row = 0
    for atr_sample, atr_sym in zip(df_ann.atr_sample.values, df_ann.atr_sym.values):
        left = max([0, (atr_sample - num_sec*fs)])
        right = min([len(p_signal), (atr_sample + num_sec*fs)])
        x = p_signal[left:right]
        if len(x) == num_cols:
            X[max_row, :] = x
            Y[max_row, :] = int(atr_sym in normal)
            sym.append(atr_sym)
            max_row += 1
    X = X[:max_row, :]
    Y = Y[:max_row, :]
    return X, Y, sym
```
**Explanation:**
- **What it does**: Creates input (`X`) and label (`Y`) matrices for heartbeats, extracting fixed-length segments around each heartbeat.
- **Parameters**:
  - `p_signal`: ECG signal (1D array, single channel).
  - `df_ann`: DataFrame with annotation symbols (`atr_sym`) and sample indices (`atr_sample`).
  - `num_cols`: Number of data points per heartbeat segment (e.g., 720 for 2 seconds at 360 Hz).
  - `normal`: List of normal beat symbols (e.g., `[“N”]`).
- **Steps**:
  - Initialize `X` (shape `[num_rows, num_cols]`) to store ECG segments and `Y` (shape `[num_rows, 1]`) for labels (0 for normal, 1 for abnormal).
  - Loop over annotations:
    - `left = max([0, (atr_sample - num_sec*fs)])`: Start index of the segment (1 second before the heartbeat).
    - `right = min([len(p_signal), (atr_sample + num_sec*fs)])`: End index (1 second after).
    - `x = p_signal[left:right]`: Extracts the segment (e.g., 720 points for 2 seconds).
    - If `len(x) == num_cols`, add to `X`, set `Y` to 0 if the symbol is normal, 1 otherwise, and store the symbol in `sym`.
  - Trim `X` and `Y` to `max_row` to remove unused rows.
- **Returns**: `X` (ECG segments), `Y` (labels), `sym` (original symbols).

**Purpose:**
This creates a dataset where each row in `X` is a fixed-length ECG segment centered on a heartbeat, and `Y` indicates if it’s normal (0) or abnormal (1).

**Example:**
Suppose `p_signal` is an array of 10,000 points, `fs = 360`, `num_sec = 1`, so `num_cols = 2*1*360 = 720`. For a heartbeat at `atr_sample = 1000` with `atr_sym = “N”`:
- `left = max(0, 1000 - 1*360) = 640`.
- `right = min(10000, 1000 + 1*360) = 1360`.
- `x = p_signal[640:1360]` (720 points).
- If `len(x) == 720`, add `x` to `X`, set `Y = 0` (since “N” is normal), and append “N” to `sym`.

**For Beginners:**
- **Why segments?** The autoencoder needs fixed-size inputs. Each segment captures the waveform of one heartbeat (1 second before and after).
- **Why normal vs. abnormal?** The model learns from normal beats and flags others as anomalies based on reconstruction error.
- **Use in PhD**: Segmenting ECG signals around heartbeats is a common preprocessing step in ECG analysis.

---

#### 12. **Creating Abnormal Beat Dataset**
```python
def make_dataset(pts, num_sec, fs, abnormal):
    num_cols = 2*num_sec * fs
    X_all = np.zeros((1, num_cols))
    Y_all = np.zeros((1, 1))
    sym_all = []
    max_rows = []
    for pt in pts:
        file = data + pt
        p_signal, atr_sym, atr_sample = load_ecg(file)
        p_signal = p_signal[:, 0]
        df_ann = pd.DataFrame({'atr_sym': atr_sym, 'atr_sample': atr_sample})
        df_ann = df_ann.loc[df_ann.atr_sym.isin(abnormal)]
        X, Y, sym = build_XY(p_signal, df_ann, num_cols, abnormal)
        sym_all = sym_all + sym
        max_rows.append(X.shape[0])
        X_all = np.append(X_all, X, axis=0)
        Y_all = np.append(Y_all, Y, axis=0)
    X_all = X_all[1:, :]
    Y_all = Y_all[1:, :]
    return X_all, Y_all, sym_all

num_sec = 1
fs = 360
X_abnormal, Y_abnormal, sym_abnormal = make_dataset(patients, num_sec, fs, abnormal)
```
**Explanation:**
- **What it does**: Creates a dataset of abnormal ECG beats from the MIT-BIH Arrhythmia Database.
- **Function `make_dataset`**:
  - **Parameters**:
    - `pts`: List of patient IDs.
    - `num_sec`: Seconds before and after each heartbeat (1 second).
    - `fs`: Sampling frequency (360 Hz).
    - `abnormal`: List of abnormal beat symbols.
  - **Steps**:
    - `num_cols = 2*num_sec*fs`: Calculates segment size (720 points for 2 seconds).
    - Initialize empty arrays `X_all`, `Y_all`, and `sym_all`.
    - For each patient:
      - Load ECG data using `load_ecg`.
      - Use the first channel (`p_signal[:, 0]`).
      - Filter annotations to include only abnormal beats (`df_ann.loc[df_ann.atr_sym.isin(abnormal)]`).
      - Call `build_XY` to get segments (`X`), labels (`Y`), and symbols (`sym`).
      - Append results to `X_all`, `Y_all`, and `sym_all`.
    - Remove the initial zero row and return the results.
- **Execution**:
  - `num_sec = 1`, `fs = 360`: Each segment is 2 seconds (720 points).
  - `X_abnormal`: Array of abnormal ECG segments.
  - `Y_abnormal`: Labels (1 for abnormal).
  - `sym_abnormal`: Original symbols (e.g., “V”, “A”).

**Purpose:**
This creates a dataset of abnormal heartbeats for training and testing the autoencoder.

**Example:**
For patient “100” with 50 “V” beats:
- Each beat is extracted as a 720-point segment.
- `X_abnormal` might have shape `[50, 720]`, `Y_abnormal` is `[50, 1]` with all 1s, and `sym_abnormal` is `[“V”, “V”, …]`.

**For Beginners:**
- **Why only one channel?** The script uses the first channel (e.g., Lead I) for simplicity, though MIT-BIH has two channels.
- **Why filter abnormal beats?** The autoencoder will compare normal and abnormal beats, so this function isolates abnormal ones.
- **Use in PhD**: This function is reusable for creating datasets for other ECG tasks (e.g., classification, denoising).

---

#### 13. **Text Cell: Normal ECG Data**
```markdown
## Normal ECG Data
data = '../input/mitbih-normal-sinus-rhythm-database/mit-bih-normal-sinus-rhythm-database-1.0.0/'
I chose two patients in order to have less computation time and less data.
patients = ["16265", "16272"]
```
**Explanation:**
- **MIT-BIH Normal Sinus Rhythm Database**: Contains ECG recordings of normal heartbeats from healthy individuals.
- **Path**: Updates `data` to point to the normal dataset folder.
- **Patient selection**: Only two patients (“16265”, “16272”) are used to reduce computation time and data size.
- **Purpose**: This dataset provides normal ECG signals to train the autoencoder, as it needs to learn the patterns of normal heartbeats to detect anomalies.

**Example:**
The normal dataset is like a collection of “perfect” heartbeats from healthy people. By using only two patients, the script keeps the dataset small for faster processing, but in a real PhD project, you might use more patients for better generalization.

**For Beginners:**
- **Why normal data?** The autoencoder learns to reconstruct normal ECGs. Abnormal ECGs will have higher errors, indicating anomalies.
- **Why only two patients?** This is a compromise for speed in a demo. In research, you’d use more data to ensure robustness.

---

#### 14. **Creating Normal Beat Dataset**
```python
def make_dataset(pts, num_sec, fs, normal):
    num_cols = 2*num_sec * fs
    X_all = np.zeros((1, num_cols))
    Y_all = np.zeros((1, 1))
    sym_all = []
    max_rows = []
    for pt in pts:
        file = data + pt
        p_signal, atr_sym, atr_sample = load_ecg(file)
        p_signal = p_signal[:, 0]
        df_ann = pd.DataFrame({'atr_sym': atr_sym, 'atr_sample': atr_sample})
        df_ann = df_ann.loc[df_ann.atr_sym.isin(normal)]
        X, Y, sym = build_XY(p_signal, df_ann, num_cols, normal)
        sym_all = sym_all + sym
        max_rows.append(X.shape[0])
        X_all = np.append(X_all, X, axis=0)
        Y_all = np.append(Y_all, Y, axis=0)
    X_all = X_all[1:, :]
    Y_all = Y_all[1:, :]
    return X_all, Y_all, sym_all

num_sec = 1
fs = 360
X_normal, Y_normal, sym_normal = make_dataset(patients, num_sec, fs, normal)
```
**Explanation:**
- **What it does**: Similar to the abnormal dataset function, this creates a dataset of normal ECG beats from the Normal Sinus Rhythm Database.
- **Function**: Identical to the previous `make_dataset`, but filters for normal beats (`normal = [“N”]`) instead of abnormal ones.
- **Execution**:
  - Uses patients “16265” and “16272”.
  - `X_normal`: Array of normal ECG segments.
  - `Y_normal`: Labels (0 for normal).
  - `sym_normal`: Symbols (all “N”).

**Purpose:**
This provides normal ECG data for training the autoencoder to reconstruct normal heartbeats accurately.

**Example:**
For patient “16265” with 2000 normal beats, each beat is a 720-point segment. `X_normal` might have shape `[2000, 720]`, `Y_normal` is `[2000, 1]` with all 0s, and `sym_normal` is `[“N”, “N”, …]`.

**For Beginners:**
- **Why separate functions?** The script reuses the same logic for normal and abnormal datasets, changing only the symbols filtered.
- **Use in PhD**: You’ll often create separate datasets for different classes (e.g., normal vs. diseased) in ECG research.

---

#### 15. **Shrinking Normal Data**
```python
X_normal = X_normal[0:34376, :]
Y_normal = np.zeros((34376, 1))
```
**Explanation:**
- **What it does**: Reduces the normal dataset to 34,376 segments and sets all labels to 0.
- **Why shrink?** The normal dataset is likely much larger than the abnormal one, causing imbalance. This step limits the number of normal samples to balance the dataset and reduce computation time.
- **Why reset `Y_normal`?** The original `Y_normal` from `make_dataset` is correct (all 0s), but this explicitly ensures consistency.

**Example:**
If `X_normal` originally has 50,000 segments, this keeps the first 34,376, resulting in `X_normal` shape `[34376, 720]` and `Y_normal` shape `[34376, 1]` with all 0s.

**For Beginners:**
- **Why balance?** Too many normal samples can bias the model. Balancing ensures fair training.
- **Use in PhD**: Handling data imbalance is a key skill in ECG research, often using techniques like undersampling (as here) or oversampling.

---

#### 16. **Combining and Preprocessing Data**
```python
X = np.append(X_normal, X_abnormal, axis=0)
Y = np.append(Y_normal, Y_abnormal, axis=0)
X = X[:, 0:140]
raw_data = np.append(X, Y, axis=1)
raw_data = pd.DataFrame(raw_data)
labels = raw_data.iloc[:, -1]
labels = labels.values
data = raw_data.iloc[:, 0:-1]
data = data.values
```
**Explanation:**
- **Combine datasets**:
  - `X = np.append(X_normal, X_abnormal, axis=0)`: Combines normal and abnormal ECG segments into one array.
  - `Y = np.append(Y_normal, Y_abnormal, axis=0)`: Combines labels (0 for normal, 1 for abnormal).
- **Reduce segment size**: `X = X[:, 0:140]`: Truncates each segment to 140 points (instead of 720), likely to focus on the QRS complex (the main heartbeat feature) and reduce computation.
- **Create DataFrame**:
  - `raw_data = np.append(X, Y, axis=1)`: Adds labels as the last column.
  - `raw_data = pd.DataFrame(raw_data)`: Converts to a DataFrame for easier manipulation.
- **Separate data and labels**:
  - `labels = raw_data.iloc[:, -1].values`: Extracts the last column (labels) as a NumPy array.
  - `data = raw_data.iloc[:, 0:-1].values`: Extracts all but the last column (ECG segments) as a NumPy array.

**Purpose:**
This prepares a unified dataset with 140-point ECG segments and binary labels (0 = normal, 1 = abnormal) for training and testing.

**Example:**
If `X_normal` is `[34376, 720]` and `X_abnormal` is `[10000, 720]`, then:
- `X` becomes `[44376, 720]` after appending.
- After `X = X[:, 0:140]`, `X` is `[44376, 140]`.
- `raw_data` is a DataFrame with 141 columns (140 for ECG data, 1 for labels).
- `data` is `[44376, 140]`, `labels` is `[44376,]` (e.g., [0, 0, …, 1, 1]).

**For Beginners:**
- **Why 140 points?** The QRS complex (main heartbeat feature) is typically ~0.4 seconds (144 points at 360 Hz). Using 140 points captures key features while reducing data size.
- **Why DataFrame?** It’s a convenient format for splitting data and labels.
- **Use in PhD**: Preprocessing ECG segments to a fixed length is standard for machine learning tasks.

---

#### 17. **Splitting and Normalizing Data**
```python
train_data, test_data, train_labels, test_labels = train_test_split(
    data, labels, test_size=0.2, random_state=21)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(train_data)
test_data = scaler.transform(test_data)
train_data = scaler.transform(train_data)
train_labels = train_labels.astype(bool)
test_labels = test_labels.astype(bool)
normal_train_data = train_data[~train_labels]
normal_test_data = test_data[~test_labels]
anomalous_train_data = train_data[train_labels]
anomalous_test_data = test_data[test_labels]
val_df, test_df = train_test_split(
    test_data, test_size=0.2, random_state=42)
test_labels = ~test_labels
```
**Explanation:**
- **Train-test split**:
  - `train_test_split(data, labels, test_size=0.2, random_state=21)`: Splits data into 80% training (`train_data`, `train_labels`) and 20% testing (`test_data`, `test_labels`).
  - `random_state=21`: Ensures reproducible splits.
- **Normalization**:
  - `MinMaxScaler`: Scales data to the range [0, 1] to help the neural network train effectively.
  - `scaler.fit(train_data)`: Learns the min and max values from the training data.
  - `scaler.transform`: Applies the scaling to both training and test data.
- **Convert labels to boolean**:
  - `train_labels = train_labels.astype(bool)`: Converts 0/1 to False/True.
  - `test_labels = test_labels.astype(bool)`: Same for test labels.
- **Separate normal and anomalous data**:
  - `normal_train_data = train_data[~train_labels]`: Selects training samples where `train_labels` is False (normal).
  - `normal_test_data = test_data[~test_labels]`: Same for test data.
  - `anomalous_train_data = train_data[train_labels]`: Selects training samples where `train_labels` is True (abnormal).
  - `anomalous_test_data = test_data[test_labels]`: Same for test data.
- **Validation split**:
  - `train_test_split(test_data, test_size=0.2, random_state=42)`: Splits test data into 80% validation (`val_df`) and 20% test (`test_df`).
- **Invert test labels**: `test_labels = ~test_labels`: Flips labels (True becomes False, False becomes True) to match the autoencoder’s convention (True for normal, False for abnormal).

**Purpose:**
This prepares training, validation, and test sets, normalizes the data, and separates normal and abnormal samples for training the autoencoder (on normal data) and evaluating it.

**Example:**
If `data` is `[44376, 140]` and `labels` is `[44376,]`, then:
- Training: ~35,500 samples, Test: ~8,876 samples.
- After scaling, values in `train_data` and `test_data` are between 0 and 1.
- `normal_train_data` contains ~28,000 normal samples, `anomalous_train_data` contains ~7,500 abnormal samples.
- `val_df` (~7,100 samples), `test_df` (~1,776 samples).
- After `~test_labels`, normal samples have `True`, abnormal have `False`.

**For Beginners:**
- **Why normalize?** Neural networks perform better with scaled data (0–1) because large values can cause training issues.
- **Why separate normal/anomalous?** The autoencoder trains on normal data to learn their patterns, then tests on both to detect anomalies.
- **Why invert labels?** The autoencoder expects normal samples to be “True” for evaluation metrics.

---

#### 18. **Visualizing ECG Data**
```python
plt.grid()
plt.plot(np.arange(140), normal_train_data[0])
plt.title("normal train data")
plt.show()

plt.grid()
plt.plot(np.arange(140), normal_test_data[543])
plt.title("normal test data")
plt.show()

plt.grid()
plt.plot(np.arange(140), anomalous_train_data[0])
plt.title("anomalous train data")
plt.show()

plt.grid()
plt.plot(np.arange(140), anomalous_test_data[0])
plt.title("anomalous test data")
plt.show()
```
**Explanation:**
- **What it does**: Plots one example each of normal and abnormal ECG segments from the training and test sets.
- **Details**:
  - `plt.grid()`: Adds a grid to the plot for readability.
  - `plt.plot(np.arange(140), normal_train_data[0])`: Plots the first normal training segment (140 points).
  - `plt.title`: Sets the plot title.
  - `plt.show()`: Displays the plot.
- **Purpose**: Visualizes the difference between normal and abnormal ECG signals to understand their shapes.
- **Output**: Four plots showing typical normal (smooth, regular QRS complex) and abnormal (irregular, distorted) ECG segments.

**Example:**
- Normal ECG plot: Shows a regular pattern with clear P, QRS, and T waves.
- Abnormal ECG plot: May show irregular peaks, missing waves, or unusual shapes (e.g., a ventricular ectopic beat).

**For Beginners:**
- **ECG Waveform**: A normal ECG has a P wave (atrial contraction), QRS complex (ventricular contraction), and T wave (ventricular relaxation). Abnormal ECGs deviate from this pattern.
- **Why visualize?** Plots help you confirm the data looks correct and understand the differences between normal and abnormal signals.
- **Use in PhD**: Visualizing ECG signals is a key step to verify preprocessing and interpret model results.

---

#### 19. **Text Cell: Building the Model**
```markdown
# Build the model
For this project, we will utilize the Autoencoder model. There are two parts to the Autoencoder architecture in general. Autoencoder model consists of dense layers with different number of neurons. The input is compressed by an encoder, and the output is decoded by a decoder.
```
**Explanation:**
- **Autoencoder Structure**:
  - **Encoder**: Compresses the 140-point ECG segment into a smaller representation (e.g., 16 values).
  - **Decoder**: Reconstructs the original 140-point segment from the compressed representation.
- **Dense Layers**: Fully connected neural network layers used in both encoder and decoder.
- **Purpose**: The autoencoder learns to reconstruct normal ECGs. High reconstruction errors indicate anomalies.

**Example:**
Think of the encoder as summarizing a book into a few key points, and the decoder as rewriting the book from those points. If the book is “normal,” the summary and rewrite are accurate. If it’s “abnormal,” the rewrite is poor, indicating an anomaly.

---

#### 20. **Defining the Autoencoder**
```python
class AnomalyDetector(Model):
    def __init__(self):
        super(AnomalyDetector, self).__init__()
        self.encoder = tf.keras.Sequential([
            layers.Dense(128, activation="relu"),
            layers.Dense(64, activation="relu"),
            layers.Dense(32, activation="relu"),
            layers.Dense(16, activation="relu"),
        ])
        self.decoder = tf.keras.Sequential([
            layers.Dense(16, activation="relu"),
            layers.Dense(32, activation="relu"),
            layers.Dense(64, activation="relu"),
            layers.Dense(128, activation="relu"),
            layers.Dense(140, activation="sigmoid")
        ])
    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

autoencoder = AnomalyDetector()
autoencoder.compile("adam", loss="mean_absolute_error")
```
**Explanation:**
- **Class Definition**:
  - `AnomalyDetector(Model)`: Creates a custom TensorFlow model by inheriting from `tf.keras.models.Model`.
  - `__init__`: Defines the encoder and decoder.
  - `encoder`: Four dense layers (140 → 128 → 64 → 32 → 16) with ReLU activation to compress the input.
  - `decoder`: Five dense layers (16 → 32 → 64 → 128 → 140) with ReLU, except the last layer uses sigmoid to output values in [0, 1] (matching normalized data).
  - `call`: Defines how data flows through the model (input → encoder → decoder → output).
- **Compilation**:
  - `adam`: An optimizer that adjusts model weights to minimize loss.
  - `mean_absolute_error`: Loss function measuring the average absolute difference between input and reconstructed output.
- **Purpose**: The autoencoder compresses ECG segments to a 16-dimensional latent space and reconstructs them, learning normal patterns.

**Example:**
For a 140-point ECG segment:
- Encoder: Compresses it to 16 values.
- Decoder: Reconstructs it back to 140 values.
- If the input is normal, the output is similar (low error). If abnormal, the output differs (high error).

**For Beginners:**
- **Dense Layer**: Connects every input to every output, transforming data (e.g., 140 inputs to 128 outputs).
- **ReLU Activation**: Sets negative values to 0, helping the model learn complex patterns.
- **Sigmoid Activation**: Outputs values between 0 and 1, suitable for normalized ECG data.
- **Use in PhD**: Autoencoders are versatile for ECG tasks like denoising or feature extraction.

---

#### 21. **Training the Autoencoder**
```python
history = autoencoder.fit(normal_train_data, normal_train_data,
                         epochs=1500, batch_size=128,
                         validation_data=(normal_test_data, normal_test_data),
                         shuffle=True)
plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
```
**Explanation:**
- **Training**:
  - `autoencoder.fit`: Trains the model on `normal_train_data`, aiming to reconstruct the same data.
  - `epochs=1500`: Runs 1500 passes through the data.
  - `batch_size=128`: Processes 128 samples at a time.
  - `validation_data`: Uses `normal_test_data` to evaluate performance during training.
  - `shuffle=True`: Randomizes the order of samples in each epoch.
- **Plotting**:
  - Plots training and validation loss over epochs to show how the model improves.

**Purpose:**
The autoencoder learns to reconstruct normal ECGs accurately. Low loss indicates good reconstruction.

**Example:**
If `normal_train_data` has 28,000 samples:
- Each epoch processes all samples in batches of 128 (~219 iterations).
- The plot shows loss decreasing, indicating the model is learning to reconstruct normal ECGs.

**For Beginners:**
- **Epoch**: One full pass through the training data.
- **Batch Size**: Number of samples processed before updating the model’s weights.
- **Loss**: Measures how different the reconstructed ECG is from the input.
- **Use in PhD**: Monitoring training and validation loss is crucial to assess model performance.

---

#### 22. **Visualizing Reconstruction**
```python
encoded_data = autoencoder.encoder(normal_test_data).numpy()
decoded_data = autoencoder.decoder(encoded_data).numpy()
plt.plot(normal_test_data[0], 'b')
plt.plot(decoded_data[0], 'r')
plt.fill_between(np.arange(140), decoded_data[0], normal_test_data[0], color='lightcoral')
plt.legend(labels=["Input", "Reconstruction", "Error"])
plt.show()
```
**Explanation:**
- **What it does**: Visualizes the input, reconstructed output, and error for a normal test ECG.
- **Steps**:
  - `encoded_data = autoencoder.encoder(normal_test_data).numpy()`: Compresses test data to the latent space.
  - `decoded_data = autoencoder.decoder(encoded_data).numpy()`: Reconstructs the data.
  - `plt.plot`: Plots the input (blue) and reconstruction (red).
  - `plt.fill_between`: Shades the difference (error) between input and reconstruction.
- **Purpose**: Shows how well the autoencoder reconstructs normal ECGs (should be very close).

**Example:**
The plot shows a normal ECG and its reconstruction overlapping closely, with a small shaded error area, indicating good performance.

**For Beginners:**
- **Why visualize?** It confirms the model reconstructs normal ECGs accurately, a sign it has learned their patterns.
- **Use in PhD**: Visualizing reconstructions helps debug and interpret model behavior.

---

#### 23. **Text Cell: Anomaly Detection**
```markdown
# Anomaly Detection
Calculate whether the reconstruction loss is larger than the defined threshold to detect abnormalities. In this section, you'll calculate the mean average error for normal cases in the training set, and then classify future examples as abnormal if the reconstruction error exceeds the training dataset's standard deviation. Plot the reconstruction error on the training set's normal ECGs.
```
**Explanation:**
- **Method**: The autoencoder reconstructs normal ECGs well (low error) but poorly for abnormal ones (high error). A threshold (mean + standard deviation of normal errors) classifies ECGs as normal or abnormal.
- **Purpose**: This introduces the anomaly detection logic, a key application of autoencoders in ECG analysis.

**Example:**
If normal ECGs have an average error of 0.05 with a standard deviation of 0.02, the threshold is 0.07. ECGs with errors above 0.07 are flagged as abnormal.

---

#### 24. **Visualizing Anomalous Reconstruction**
```python
encoded_data = autoencoder.encoder(anomalous_test_data).numpy()
decoded_data = autoencoder.decoder(encoded_data).numpy()
plt.plot(anomalous_test_data[0], 'b')
plt.plot(decoded_data[0], 'r')
plt.fill_between(np.arange(140), decoded_data[0], anomalous_test_data[0], color='lightcoral')
plt.legend(labels=["Input", "Reconstruction", "Error"])
plt.show()
```
**Explanation:**
- Similar to the normal reconstruction plot, but for an abnormal test ECG.
- The reconstruction is likely poor, with a larger shaded error area, indicating the autoencoder struggles with abnormal patterns.

**Example:**
The plot shows the abnormal ECG (blue) and a mismatched reconstruction (red), with a large error area, confirming the model detects anomalies.

---

#### 25. **Calculating Threshold and Visualizing Errors**
```python
reconstructions = autoencoder.predict(normal_train_data)
train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)
plt.hist(train_loss[None, :], bins=50)
plt.xlabel("Train loss")
plt.ylabel("No of examples")
plt.show()
threshold = np.mean(train_loss) + np.std(train_loss)
print("Threshold: ", threshold)
```
**Explanation:**
- **Calculate errors**: `train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)` computes the mean absolute error (MAE) for each normal training sample.
- **Visualize**: Plots a histogram of errors to show their distribution.
- **Threshold**: `threshold = np.mean(train_loss) + np.std(train_loss)` sets the anomaly threshold as the mean error plus one standard deviation.
- **Purpose**: The threshold determines which ECGs are normal (error ≤ threshold) or abnormal (error > threshold).

**Example:**
If `train_loss` has a mean of 0.05 and standard deviation of 0.02, `threshold = 0.07`. The histogram shows most normal errors are below 0.07.

**For Beginners:**
- **MAE**: Average absolute difference between input and reconstruction.
- **Threshold**: A cutoff to decide if an ECG is abnormal based on reconstruction error.
- **Use in PhD**: Threshold-based anomaly detection is common in unsupervised ECG analysis.

---

#### 26. **Anomalous Error Distribution**
```python
reconstructions = autoencoder.predict(anomalous_test_data)
test_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)
plt.hist(test_loss[None, :], bins=50)
plt.xlabel("Test loss")
plt.ylabel("No of examples")
plt.show()
```
**Explanation:**
- Computes MAE for anomalous test data and plots a histogram.
- **Purpose**: Shows that abnormal ECGs have higher reconstruction errors, often exceeding the threshold.

**Example:**
The histogram shows most anomalous errors are above the threshold (e.g., >0.07), confirming the model detects abnormalities.

---

#### 27. **Prediction and Evaluation**
```python
def predict(model, data, threshold):
    reconstructions = model(data)
    loss = tf.keras.losses.mae(reconstructions, data)
    return tf.math.less(loss, threshold)

def print_stats(predictions, labels):
    print("Accuracy = {}".format(accuracy_score(labels, predictions)))
    print("Precision = {}".format(precision_score(labels, predictions)))
    print("Recall = {}".format(recall_score(labels, predictions)))

preds = predict(autoencoder, test_data, threshold)
print_stats(preds, test_labels)
```
**Explanation:**
- **Predict function**:
  - Computes reconstructions and MAE for input data.
  - Returns `True` (normal) if error ≤ threshold, `False` (abnormal) otherwise.
- **Print stats**:
  - `accuracy_score`: Fraction of correct predictions.
  - `precision_score`: Fraction of predicted anomalies that are actually abnormal.
  - `recall_score`: Fraction of actual anomalies correctly predicted.
- **Execution**: Evaluates the model on `test_data` and prints metrics.

**Example:**
If `preds` is `[True, True, False, …]` and `test_labels` is `[True, True, False, …]`, metrics might be:
```
Accuracy = 0.95
Precision = 0.93
Recall = 0.90
```

**For Beginners:**
- **Accuracy**: How often the model is correct.
- **Precision**: How reliable the model’s anomaly predictions are.
- **Recall**: How well the model finds all anomalies.
- **Use in PhD**: These metrics are standard for evaluating ECG models.

---

#### 28. **Error Distribution Plot**
```python
reconstructions = autoencoder.predict(anomalous_test_data)
train_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)
sns.distplot(train_loss, bins=50, kde=True)
```
**Explanation:**
- Computes MAE for anomalous test data and plots a distribution with a kernel density estimate (KDE) using `seaborn`.
- **Purpose**: Visualizes the spread of errors for anomalous data, typically higher than normal errors.

**Example:**
The plot shows a right-skewed distribution with errors mostly above the threshold, reinforcing anomaly detection.

---

#### 29. **Counting Correct Predictions**
```python
reconstructions = autoencoder.predict(normal_test_data)
pred_loss = tf.keras.losses.mae(reconstructions, normal_test_data)
pred_loss = pred_loss.numpy()
correct = sum(l <= threshold for l in pred_loss)
print(f'Correct normal predictions: {correct}/{len(normal_test_data)}')

reconstructions = autoencoder.predict(anomalous_test_data)
train_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)
train_loss = train_loss.numpy()
correct = sum(l > threshold for l in train_loss)
print(f'Correct anomaly predictions: {correct}/{len(anomalous_test_data)}')
```
**Explanation:**
- Counts how many normal test samples have errors ≤ threshold (correctly classified as normal).
- Counts how many anomalous test samples have errors > threshold (correctly classified as abnormal).
- **Purpose**: Quantifies the model’s performance in distinguishing normal and abnormal ECGs.

**Example:**
Output might be:
```
Correct normal predictions: 6700/7000
Correct anomaly predictions: 1600/1776
```

**For Beginners:**
- **Why count?** It gives a simple measure of how well the model separates normal and abnormal ECGs.
- **Use in PhD**: Such counts help validate your model’s effectiveness in reports.

---

#### 30. **Visualizing Predictions**
```python
def plot_prediction_normal(i, data, model, title, ax):
    encoded_data = autoencoder.encoder(data).numpy()
    decoded_data = autoencoder.decoder(encoded_data).numpy()
    ax.axis([0, 140, 0, 1])
    ax.plot(data[i], label='true')
    ax.plot(decoded_data[i], label='reconstructed')
    ax.set_title(f'{title} (loss: {np.around(1000*pred_loss[i], 2)})')
    ax.legend()

def plot_prediction_anomaly(i, data, model, title, ax):
    encoded_data = autoencoder.encoder(data).numpy()
    decoded_data = autoencoder.decoder(encoded_data).numpy()
    ax.axis([0, 140, 0, 1])
    ax.plot(data[i], label='true')
    ax.plot(decoded_data[i], label='reconstructed')
    ax.set_title(f'{title} (loss: {np.around(1000*train_loss[i], 2)})')
    ax.legend()

fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(22, 8))
for i in range(5):
    plot_prediction_normal(i, normal_test_data, autoencoder, title='Normal', ax=axs[0, i])
for i in range(5):
    plot_prediction_anomaly(i, anomalous_test_data, autoencoder, title='Anomaly', ax=axs[1, i])
fig.tight_layout()
```
**Explanation:**
- **Functions**: Plot true vs. reconstructed ECGs for normal and anomalous samples, showing the loss.
- **Plotting**:
  - Creates a 2×5 grid of plots.
  - Top row: Five normal test ECGs.
  - Bottom row: Five anomalous test ECGs.
  - Each plot shows the true signal, reconstructed signal, and MAE (in milli-units).
- **Purpose**: Visualizes how well the autoencoder reconstructs normal vs. abnormal ECGs.

**Example:**
Normal plots show close overlap (low loss, e.g., 50), while anomalous plots show larger differences (high loss, e.g., 200).

**For Beginners:**
- **Why multiple plots?** Comparing multiple examples helps confirm consistent performance.
- **Use in PhD**: Such visualizations are common in papers to demonstrate model behavior.

---

#### 31. **Text Cell: Conclusion**
```markdown
This project provides the results of an unsupervised anomaly detection for ECG data which is performed on MIT-BIH Arrhythmia Dataset and MIT-BIH Normal Sinus Rhythm Dataset. Although the findings are promising, there is certainly potential for improvement. Future projects might include: more complex model and different type of wavelet and more data to detect anomalies.
```
**Explanation:**
- **Summary**: The project successfully detects ECG anomalies using an autoencoder, achieving good results.
- **Future Work**:
  - **More complex model**: E.g., convolutional autoencoders for better feature extraction.
  - **Wavelet**: Using wavelet transforms for preprocessing ECGs.
  - **More data**: Including more patients for robustness.
- **Purpose**: Highlights the project’s achievements and suggests improvements for future research.
- **For your PhD**: This outlines a typical conclusion for an ECG study, emphasizing contributions and future directions.

**Example:**
The project is like a first draft of a novel. It tells a good story (detects anomalies), but you could add more chapters (data), improve the writing (model), or use new tools (wavelets).
